{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validação Cruzada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validação cruzada é você usar diversas divisões de base, usando diversas possibilidades pra encontrar quais são os dados previsores que melhor funcionam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etapa 1: Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from skorch import NeuralNetBinaryClassifier #pytorch nao tem validacao cruzada\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etapa 2: Base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21e910237d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123) # Mesmos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = pd.read_csv('Arquivos/entradas_breast.csv')\n",
    "classes = pd.read_csv('Arquivos/saidas_breast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = np.array(previsores, dtype = 'float32')\n",
    "classes = np.array(classes, dtype='float32').squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etapa 3: Classe para estrutura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classificador_torch(nn.Module):\n",
    "    def __init__(self): ## Construtor\n",
    "        super().__init__() ## Ta pegando todas as características da classe nn.Module\n",
    "\n",
    "        # 30 -> 16 -> 16 -> 1\n",
    "        # Atributos\n",
    "        self.dense0 = nn.Linear(30,16)\n",
    "        torch.nn.init.uniform_(self.dense0.weight) # Inicializa Pesos uniformes\n",
    "        self.activation0 = nn.ReLU()\n",
    "        self.dense1 = nn.Linear(16,16)\n",
    "        torch.nn.init.uniform_(self.dense1.weight)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(16,1)\n",
    "        torch.nn.init.uniform_(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X): # X são as entradas\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etapa 4: Skorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador_sklearn = NeuralNetBinaryClassifier(module=classificador_torch, \n",
    "                                                  criterion=torch.nn.BCELoss, \n",
    "                                                  optimizer = torch.optim.Adam, \n",
    "                                                  lr = 0.001, \n",
    "                                                  optimizer__weight_decay = 0.0001,\n",
    "                                                  max_epochs=100,\n",
    "                                                  batch_size = 10,\n",
    "                                                  train_split=False) # Train split é falso porque faremos a validação cruzada\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etapa 5: Validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     89       37.3047  0.0660\n",
      "     90       37.3047  0.0720\n",
      "     91       37.3047  0.1040\n",
      "     92       37.3047  0.0730\n",
      "     93       37.3047  0.0790\n",
      "     94       37.3047  0.0750\n",
      "     95       37.3047  0.0680\n",
      "     96       37.3047  0.0680\n",
      "     97       37.3047  0.0650\n",
      "     98       37.3047  0.0690\n",
      "     99       37.3047  0.0840\n",
      "    100       37.3047  0.0780\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2320\u001b[0m  0.0680\n",
      "      2       37.2320  0.0630\n",
      "      3       37.2320  0.0620\n",
      "      4       37.2320  0.0650\n",
      "      5       37.2320  0.0680\n",
      "      6       37.2320  0.0700\n",
      "      7       37.2320  0.0670\n",
      "      8       37.2320  0.0620\n",
      "      9       37.2320  0.0625\n",
      "     10       37.2320  0.0620\n",
      "     11       37.2320  0.0600\n",
      "     12       37.2320  0.0630\n",
      "     13       37.2320  0.0620\n",
      "     14       37.2320  0.0610\n",
      "     15       37.2320  0.0620\n",
      "     16       37.2320  0.0650\n",
      "     17       37.2320  0.0640\n",
      "     18       37.2320  0.0730\n",
      "     19       37.2320  0.0660\n",
      "     20       37.2320  0.0680\n",
      "     21       37.2320  0.0630\n",
      "     22       37.2320  0.0630\n",
      "     23       37.2320  0.0620\n",
      "     24       37.2320  0.0620\n",
      "     25       37.2320  0.0640\n",
      "     26       \u001b[36m17.4384\u001b[0m  0.0650\n",
      "     27        \u001b[36m0.6449\u001b[0m  0.0620\n",
      "     28        \u001b[36m0.6204\u001b[0m  0.0630\n",
      "     29        \u001b[36m0.6185\u001b[0m  0.0650\n",
      "     30        \u001b[36m0.6175\u001b[0m  0.0650\n",
      "     31        \u001b[36m0.6167\u001b[0m  0.0640\n",
      "     32        \u001b[36m0.6161\u001b[0m  0.0610\n",
      "     33        \u001b[36m0.6156\u001b[0m  0.0620\n",
      "     34        \u001b[36m0.6152\u001b[0m  0.0650\n",
      "     35        \u001b[36m0.6149\u001b[0m  0.0660\n",
      "     36        \u001b[36m0.6147\u001b[0m  0.0600\n",
      "     37        \u001b[36m0.6144\u001b[0m  0.0630\n",
      "     38        \u001b[36m0.6142\u001b[0m  0.0590\n",
      "     39        \u001b[36m0.6140\u001b[0m  0.0650\n",
      "     40        \u001b[36m0.6139\u001b[0m  0.0640\n",
      "     41        \u001b[36m0.6138\u001b[0m  0.0640\n",
      "     42        \u001b[36m0.6137\u001b[0m  0.0680\n",
      "     43        \u001b[36m0.6136\u001b[0m  0.0710\n",
      "     44        \u001b[36m0.6135\u001b[0m  0.0630\n",
      "     45        \u001b[36m0.6134\u001b[0m  0.0650\n",
      "     46        \u001b[36m0.6134\u001b[0m  0.0620\n",
      "     47        \u001b[36m0.6133\u001b[0m  0.0640\n",
      "     48        \u001b[36m0.6133\u001b[0m  0.0720\n",
      "     49        \u001b[36m0.6132\u001b[0m  0.0620\n",
      "     50        \u001b[36m0.6132\u001b[0m  0.0640\n",
      "     51        \u001b[36m0.6132\u001b[0m  0.0650\n",
      "     52        \u001b[36m0.6131\u001b[0m  0.0630\n",
      "     53        \u001b[36m0.6131\u001b[0m  0.0620\n",
      "     54        \u001b[36m0.6131\u001b[0m  0.0630\n",
      "     55        \u001b[36m0.6131\u001b[0m  0.0660\n",
      "     56        \u001b[36m0.6131\u001b[0m  0.0690\n",
      "     57        \u001b[36m0.6130\u001b[0m  0.0630\n",
      "     58        \u001b[36m0.6130\u001b[0m  0.0610\n",
      "     59        \u001b[36m0.6130\u001b[0m  0.0610\n",
      "     60        \u001b[36m0.6130\u001b[0m  0.0690\n",
      "     61        \u001b[36m0.6130\u001b[0m  0.0790\n",
      "     62        \u001b[36m0.6130\u001b[0m  0.0800\n",
      "     63        \u001b[36m0.6130\u001b[0m  0.0700\n",
      "     64        \u001b[36m0.6130\u001b[0m  0.0690\n",
      "     65        \u001b[36m0.6130\u001b[0m  0.0700\n",
      "     66        \u001b[36m0.6130\u001b[0m  0.0740\n",
      "     67        \u001b[36m0.6130\u001b[0m  0.0710\n",
      "     68        \u001b[36m0.6130\u001b[0m  0.0680\n",
      "     69        \u001b[36m0.6130\u001b[0m  0.0680\n",
      "     70        \u001b[36m0.6130\u001b[0m  0.0680\n",
      "     71        \u001b[36m0.6130\u001b[0m  0.0660\n",
      "     72        \u001b[36m0.6130\u001b[0m  0.0660\n",
      "     73        \u001b[36m0.6130\u001b[0m  0.0610\n",
      "     74        \u001b[36m0.6130\u001b[0m  0.0630\n",
      "     75        \u001b[36m0.6130\u001b[0m  0.0600\n",
      "     76        0.6130  0.0620\n",
      "     77        0.6130  0.0620\n",
      "     78        0.6130  0.0610\n",
      "     79        0.6130  0.0590\n",
      "     80        0.6130  0.0640\n",
      "     81        0.6130  0.0630\n",
      "     82        0.6130  0.0640\n",
      "     83        0.6130  0.0680\n",
      "     84        0.6130  0.0620\n",
      "     85        0.6130  0.0630\n",
      "     86        0.6130  0.0610\n",
      "     87        0.6130  0.0620\n",
      "     88        0.6130  0.0620\n",
      "     89        0.6130  0.0950\n",
      "     90        0.6130  0.0700\n",
      "     91        0.6130  0.0650\n",
      "     92        0.6130  0.0660\n",
      "     93        0.6130  0.0680\n",
      "     94        0.6130  0.0680\n",
      "     95        0.6130  0.0700\n",
      "     96        0.6130  0.0640\n",
      "     97        0.6130  0.0610\n",
      "     98        0.6130  0.0630\n",
      "     99        0.6130  0.0630\n",
      "    100        0.6130  0.0600\n"
     ]
    }
   ],
   "source": [
    "resultados = cross_val_score(classificador_sklearn, previsores, classes, cv = 10, scoring = 'accuracy') # Cross validation = cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7835526315789474, 0.13066955478793235)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media = resultados.mean()\n",
    "desvio = resultados.std()\n",
    "media, desvio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84210526, 0.61403509, 0.89473684, 0.63157895, 0.89473684,\n",
       "       0.89473684, 0.87719298, 0.92982456, 0.63157895, 0.625     ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etapa 6: Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previne Overfitting fazendo com que alguns neurônios sejam desligados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classificador_torch(nn.Module):\n",
    "    def __init__(self): ## Construtor\n",
    "        super().__init__() ## Ta pegando todas as características da classe nn.Module\n",
    "\n",
    "        # 30 -> 16 -> 16 -> 1\n",
    "        # Atributos\n",
    "        self.dense0 = nn.Linear(30,16)\n",
    "        torch.nn.init.uniform_(self.dense0.weight) # Inicializa Pesos uniformes\n",
    "        self.activation0 = nn.ReLU()\n",
    "        self.dropout0 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(16,16)\n",
    "        torch.nn.init.uniform_(self.dense1.weight)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense2 = nn.Linear(16,1)\n",
    "        torch.nn.init.uniform_(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X): # X são as entradas\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dropout0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dropout1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador_sklearn = NeuralNetBinaryClassifier(module=classificador_torch, \n",
    "                                                  criterion=torch.nn.BCELoss, \n",
    "                                                  optimizer = torch.optim.Adam, \n",
    "                                                  lr = 0.001, \n",
    "                                                  optimizer__weight_decay = 0.0001,\n",
    "                                                  max_epochs=100,\n",
    "                                                  batch_size = 10,\n",
    "                                                  train_split=False) # Train split é falso porque faremos a validação cruzada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13       37.1094  0.0740\n",
      "     14       37.1094  0.0780\n",
      "     15       37.1094  0.0780\n",
      "     16       37.1094  0.0910\n",
      "     17       37.1094  0.0870\n",
      "     18       37.1094  0.0900\n",
      "     19       37.1094  0.0870\n",
      "     20       37.1094  0.0750\n",
      "     21       \u001b[36m36.9392\u001b[0m  0.0730\n",
      "     22       37.1149  0.0700\n",
      "     23       36.9493  0.0710\n",
      "     24       \u001b[36m13.8035\u001b[0m  0.0680\n",
      "     25        \u001b[36m0.6964\u001b[0m  0.0710\n",
      "     26        0.6974  0.0680\n",
      "     27        \u001b[36m0.6398\u001b[0m  0.0670\n",
      "     28        \u001b[36m0.4511\u001b[0m  0.0670\n",
      "     29        0.4559  0.0680\n",
      "     30        \u001b[36m0.4056\u001b[0m  0.0720\n",
      "     31        \u001b[36m0.4035\u001b[0m  0.0720\n",
      "     32        \u001b[36m0.3692\u001b[0m  0.0670\n",
      "     33        0.3894  0.0720\n",
      "     34        \u001b[36m0.3629\u001b[0m  0.0710\n",
      "     35        \u001b[36m0.3445\u001b[0m  0.0760\n",
      "     36        \u001b[36m0.3443\u001b[0m  0.0740\n",
      "     37        \u001b[36m0.3421\u001b[0m  0.0700\n",
      "     38        \u001b[36m0.3141\u001b[0m  0.0710\n",
      "     39        0.3162  0.0800\n",
      "     40        0.3161  0.0700\n",
      "     41        0.3206  0.0670\n",
      "     42        \u001b[36m0.3043\u001b[0m  0.0760\n",
      "     43        \u001b[36m0.2993\u001b[0m  0.0770\n",
      "     44        \u001b[36m0.2708\u001b[0m  0.0670\n",
      "     45        0.3039  0.0690\n",
      "     46        0.2774  0.0770\n",
      "     47        0.2768  0.0710\n",
      "     48        \u001b[36m0.2284\u001b[0m  0.0690\n",
      "     49        0.2973  0.0740\n",
      "     50        0.2647  0.0790\n",
      "     51        0.2533  0.0690\n",
      "     52        0.2512  0.0630\n",
      "     53        0.2596  0.0690\n",
      "     54        0.2337  0.0690\n",
      "     55        \u001b[36m0.2269\u001b[0m  0.0680\n",
      "     56        0.2322  0.0670\n",
      "     57        0.2360  0.0660\n",
      "     58        0.2415  0.0680\n",
      "     59        0.2472  0.0670\n",
      "     60        \u001b[36m0.2234\u001b[0m  0.0660\n",
      "     61        0.2373  0.0690\n",
      "     62        0.2266  0.0740\n",
      "     63        0.2343  0.1120\n",
      "     64        \u001b[36m0.2159\u001b[0m  0.0690\n",
      "     65        0.2188  0.0710\n",
      "     66        0.2359  0.0650\n",
      "     67        0.2271  0.0680\n",
      "     68        \u001b[36m0.2017\u001b[0m  0.0870\n",
      "     69        0.2299  0.0660\n",
      "     70        0.2297  0.0680\n",
      "     71        0.2211  0.0660\n",
      "     72        0.2342  0.0700\n",
      "     73        0.2140  0.0680\n",
      "     74        0.2363  0.0670\n",
      "     75        0.2196  0.0700\n",
      "     76        0.2026  0.0660\n",
      "     77        0.2119  0.0690\n",
      "     78        \u001b[36m0.1876\u001b[0m  0.0720\n",
      "     79        0.2114  0.0690\n",
      "     80        0.2095  0.0680\n",
      "     81        0.2453  0.0630\n",
      "     82        0.2214  0.0660\n",
      "     83        0.2298  0.0730\n",
      "     84        \u001b[36m0.1820\u001b[0m  0.0720\n",
      "     85        0.2250  0.0670\n",
      "     86        0.2224  0.0650\n",
      "     87        0.2303  0.0690\n",
      "     88        0.2340  0.0680\n",
      "     89        0.2237  0.0680\n",
      "     90        0.2266  0.0660\n",
      "     91        0.2493  0.0700\n",
      "     92        0.2680  0.0650\n",
      "     93        0.2520  0.0680\n",
      "     94        0.2170  0.0670\n",
      "     95        0.2237  0.0820\n",
      "     96        0.2003  0.0710\n",
      "     97        0.2152  0.0680\n",
      "     98        0.2164  0.0700\n",
      "     99        0.2135  0.0700\n",
      "    100        0.2173  0.0780\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0800\n",
      "      2       37.3047  0.0680\n",
      "      3       37.3047  0.0690\n",
      "      4       37.3047  0.0640\n",
      "      5       37.3047  0.0770\n",
      "      6       37.3047  0.0760\n",
      "      7       37.3047  0.0680\n",
      "      8       37.3047  0.0670\n",
      "      9       37.3047  0.0660\n",
      "     10       37.3047  0.0780\n",
      "     11       37.3047  0.0730\n",
      "     12       37.3047  0.0670\n",
      "     13       37.3047  0.0690\n",
      "     14       37.3047  0.0650\n",
      "     15       37.3047  0.0680\n",
      "     16       37.3047  0.0670\n",
      "     17       37.3047  0.0670\n",
      "     18       37.3047  0.0670\n",
      "     19       37.3047  0.0670\n",
      "     20       37.3047  0.0710\n",
      "     21       37.3047  0.0720\n",
      "     22       37.3047  0.0690\n",
      "     23       37.3047  0.0670\n",
      "     24       37.3047  0.0690\n",
      "     25       37.3047  0.0690\n",
      "     26       \u001b[36m37.1392\u001b[0m  0.0670\n",
      "     27       37.3047  0.0670\n",
      "     28       37.3047  0.0690\n",
      "     29       \u001b[36m30.0540\u001b[0m  0.0690\n",
      "     30        \u001b[36m0.7032\u001b[0m  0.0690\n",
      "     31        \u001b[36m0.5904\u001b[0m  0.0670\n",
      "     32        \u001b[36m0.5593\u001b[0m  0.0670\n",
      "     33        \u001b[36m0.5330\u001b[0m  0.0670\n",
      "     34        0.5474  0.0710\n",
      "     35        0.5333  0.0660\n",
      "     36        \u001b[36m0.5023\u001b[0m  0.0680\n",
      "     37        0.5054  0.0670\n",
      "     38        0.5176  0.0710\n",
      "     39        \u001b[36m0.4938\u001b[0m  0.0680\n",
      "     40        0.4989  0.0690\n",
      "     41        \u001b[36m0.4893\u001b[0m  0.0680\n",
      "     42        \u001b[36m0.4738\u001b[0m  0.0670\n",
      "     43        \u001b[36m0.4605\u001b[0m  0.0660\n",
      "     44        0.4617  0.0660\n",
      "     45        \u001b[36m0.4495\u001b[0m  0.0680\n",
      "     46        \u001b[36m0.4253\u001b[0m  0.0680\n",
      "     47        \u001b[36m0.4154\u001b[0m  0.0660\n",
      "     48        0.4260  0.0740\n",
      "     49        \u001b[36m0.3989\u001b[0m  0.0690\n",
      "     50        0.4099  0.0700\n",
      "     51        \u001b[36m0.3910\u001b[0m  0.0680\n",
      "     52        0.4082  0.0690\n",
      "     53        \u001b[36m0.3676\u001b[0m  0.0760\n",
      "     54        \u001b[36m0.3468\u001b[0m  0.0820\n",
      "     55        0.3574  0.0790\n",
      "     56        0.3765  0.0770\n",
      "     57        0.3809  0.0750\n",
      "     58        0.3470  0.0810\n",
      "     59        \u001b[36m0.3261\u001b[0m  0.0780\n",
      "     60        0.3499  0.1100\n",
      "     61        0.3489  0.0740\n",
      "     62        0.3407  0.0770\n",
      "     63        0.3374  0.0710\n",
      "     64        0.3269  0.0680\n",
      "     65        0.3557  0.0700\n",
      "     66        0.3591  0.0730\n",
      "     67        0.3398  0.0690\n",
      "     68        0.3327  0.0670\n",
      "     69        \u001b[36m0.3257\u001b[0m  0.0660\n",
      "     70        0.3307  0.0640\n",
      "     71        0.3412  0.0660\n",
      "     72        0.3450  0.0660\n",
      "     73        \u001b[36m0.3207\u001b[0m  0.0660\n",
      "     74        0.3442  0.0660\n",
      "     75        0.3243  0.0650\n",
      "     76        0.3455  0.0690\n",
      "     77        \u001b[36m0.3023\u001b[0m  0.0670\n",
      "     78        0.3181  0.0680\n",
      "     79        0.3118  0.0640\n",
      "     80        \u001b[36m0.2859\u001b[0m  0.0690\n",
      "     81        0.3076  0.0680\n",
      "     82        0.3031  0.0670\n",
      "     83        0.2951  0.0660\n",
      "     84        0.3167  0.0680\n",
      "     85        0.2952  0.0680\n",
      "     86        0.3213  0.0660\n",
      "     87        0.3202  0.0680\n",
      "     88        0.3215  0.0680\n",
      "     89        0.3152  0.0660\n",
      "     90        \u001b[36m0.2845\u001b[0m  0.0640\n",
      "     91        0.2888  0.0640\n",
      "     92        0.2889  0.0870\n",
      "     93        \u001b[36m0.2750\u001b[0m  0.0730\n",
      "     94        0.3064  0.0810\n",
      "     95        0.3471  0.0770\n",
      "     96        0.3112  0.0750\n",
      "     97        0.2779  0.0910\n",
      "     98        0.3091  0.0810\n",
      "     99        0.3131  0.0860\n",
      "    100        0.3350  0.0885\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0911\n",
      "      2       37.3047  0.0810\n",
      "      3       37.3047  0.0770\n",
      "      4       37.3047  0.0700\n",
      "      5       37.3047  0.0660\n",
      "      6       37.3047  0.0740\n",
      "      7       37.3047  0.0750\n",
      "      8       37.3047  0.0650\n",
      "      9       37.3047  0.0630\n",
      "     10       37.3047  0.0670\n",
      "     11       37.3047  0.0660\n",
      "     12       37.3047  0.0660\n",
      "     13       37.3047  0.0710\n",
      "     14       37.3047  0.0750\n",
      "     15       37.3047  0.0680\n",
      "     16       37.3047  0.0640\n",
      "     17       37.3047  0.0650\n",
      "     18       37.3047  0.0660\n",
      "     19       37.3047  0.0700\n",
      "     20       37.3047  0.0720\n",
      "     21       37.3047  0.0720\n",
      "     22       37.3047  0.0730\n",
      "     23       37.3047  0.0750\n",
      "     24       37.3047  0.0760\n",
      "     25       37.3047  0.0710\n",
      "     26       \u001b[36m37.1294\u001b[0m  0.0840\n",
      "     27        \u001b[36m9.1390\u001b[0m  0.0860\n",
      "     28        \u001b[36m0.6309\u001b[0m  0.1130\n",
      "     29        \u001b[36m0.5966\u001b[0m  0.0680\n",
      "     30        \u001b[36m0.5631\u001b[0m  0.0690\n",
      "     31        0.5914  0.0780\n",
      "     32        \u001b[36m0.5583\u001b[0m  0.0780\n",
      "     33        \u001b[36m0.5348\u001b[0m  0.0860\n",
      "     34        \u001b[36m0.5268\u001b[0m  0.0830\n",
      "     35        \u001b[36m0.4864\u001b[0m  0.0820\n",
      "     36        0.4873  0.0770\n",
      "     37        \u001b[36m0.4562\u001b[0m  0.0790\n",
      "     38        \u001b[36m0.4551\u001b[0m  0.0730\n",
      "     39        \u001b[36m0.4520\u001b[0m  0.0710\n",
      "     40        \u001b[36m0.4354\u001b[0m  0.0710\n",
      "     41        \u001b[36m0.4326\u001b[0m  0.0660\n",
      "     42        \u001b[36m0.4159\u001b[0m  0.0730\n",
      "     43        \u001b[36m0.4082\u001b[0m  0.0650\n",
      "     44        \u001b[36m0.4077\u001b[0m  0.0670\n",
      "     45        0.4201  0.0640\n",
      "     46        \u001b[36m0.3984\u001b[0m  0.0640\n",
      "     47        \u001b[36m0.3959\u001b[0m  0.0680\n",
      "     48        \u001b[36m0.3762\u001b[0m  0.0680\n",
      "     49        0.3861  0.0650\n",
      "     50        \u001b[36m0.3648\u001b[0m  0.0620\n",
      "     51        0.3990  0.0620\n",
      "     52        \u001b[36m0.3629\u001b[0m  0.0620\n",
      "     53        \u001b[36m0.3581\u001b[0m  0.0620\n",
      "     54        \u001b[36m0.3457\u001b[0m  0.0659\n",
      "     55        0.3515  0.0620\n",
      "     56        \u001b[36m0.3441\u001b[0m  0.0640\n",
      "     57        0.3569  0.0630\n",
      "     58        0.3515  0.0640\n",
      "     59        \u001b[36m0.3135\u001b[0m  0.0660\n",
      "     60        0.3566  0.0700\n",
      "     61        0.3501  0.0700\n",
      "     62        0.3439  0.0680\n",
      "     63        0.3510  0.0690\n",
      "     64        0.3348  0.0720\n",
      "     65        0.3449  0.0730\n",
      "     66        0.3339  0.0800\n",
      "     67        0.3346  0.0720\n",
      "     68        0.3217  0.0736\n",
      "     69        0.3287  0.0690\n",
      "     70        0.3137  0.0680\n",
      "     71        \u001b[36m0.3115\u001b[0m  0.0670\n",
      "     72        0.3355  0.0650\n",
      "     73        0.3249  0.0680\n",
      "     74        \u001b[36m0.2921\u001b[0m  0.0660\n",
      "     75        0.3038  0.0630\n",
      "     76        0.3139  0.0640\n",
      "     77        0.3249  0.0620\n",
      "     78        \u001b[36m0.2897\u001b[0m  0.0640\n",
      "     79        0.3057  0.0630\n",
      "     80        0.2948  0.0670\n",
      "     81        0.3015  0.0710\n",
      "     82        \u001b[36m0.2879\u001b[0m  0.0700\n",
      "     83        \u001b[36m0.2727\u001b[0m  0.0740\n",
      "     84        0.2904  0.0730\n",
      "     85        0.3054  0.0650\n",
      "     86        0.2811  0.0600\n",
      "     87        0.2984  0.0670\n",
      "     88        0.2727  0.0630\n",
      "     89        0.2798  0.0630\n",
      "     90        \u001b[36m0.2621\u001b[0m  0.0660\n",
      "     91        0.2892  0.0640\n",
      "     92        \u001b[36m0.2571\u001b[0m  0.0700\n",
      "     93        0.2723  0.0690\n",
      "     94        \u001b[36m0.2570\u001b[0m  0.0670\n",
      "     95        0.2728  0.0690\n",
      "     96        \u001b[36m0.2512\u001b[0m  0.0660\n",
      "     97        0.2790  0.0630\n",
      "     98        0.2800  0.0640\n",
      "     99        0.2772  0.0660\n",
      "    100        0.2707  0.0640\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0630\n",
      "      2       37.3047  0.0590\n",
      "      3       37.3047  0.0660\n",
      "      4       37.3047  0.0650\n",
      "      5       37.3047  0.0640\n",
      "      6       37.3047  0.0730\n",
      "      7       37.3047  0.0660\n",
      "      8       37.3047  0.0660\n",
      "      9       37.3047  0.0650\n",
      "     10       37.3047  0.0620\n",
      "     11       37.3047  0.0660\n",
      "     12       37.3047  0.0680\n",
      "     13       37.3047  0.0640\n",
      "     14       37.3047  0.0620\n",
      "     15       37.3047  0.0610\n",
      "     16       37.3047  0.0630\n",
      "     17       37.3047  0.0630\n",
      "     18       37.3047  0.0640\n",
      "     19       37.3047  0.0600\n",
      "     20       37.3047  0.0600\n",
      "     21       37.3047  0.0660\n",
      "     22       37.3047  0.0660\n",
      "     23       37.3047  0.0650\n",
      "     24       37.3047  0.0660\n",
      "     25       37.3047  0.0630\n",
      "     26       37.3047  0.0600\n",
      "     27       \u001b[36m18.3439\u001b[0m  0.0640\n",
      "     28        \u001b[36m0.5968\u001b[0m  0.0590\n",
      "     29        \u001b[36m0.5474\u001b[0m  0.0620\n",
      "     30        \u001b[36m0.5293\u001b[0m  0.0640\n",
      "     31        \u001b[36m0.5259\u001b[0m  0.0620\n",
      "     32        \u001b[36m0.5187\u001b[0m  0.0640\n",
      "     33        \u001b[36m0.5052\u001b[0m  0.0620\n",
      "     34        \u001b[36m0.4958\u001b[0m  0.0600\n",
      "     35        \u001b[36m0.4646\u001b[0m  0.0620\n",
      "     36        \u001b[36m0.4559\u001b[0m  0.0630\n",
      "     37        \u001b[36m0.4389\u001b[0m  0.0620\n",
      "     38        \u001b[36m0.4261\u001b[0m  0.0640\n",
      "     39        \u001b[36m0.4217\u001b[0m  0.0650\n",
      "     40        0.4253  0.0660\n",
      "     41        \u001b[36m0.4072\u001b[0m  0.0660\n",
      "     42        \u001b[36m0.3968\u001b[0m  0.0660\n",
      "     43        0.4010  0.0630\n",
      "     44        0.4040  0.0600\n",
      "     45        0.3989  0.0660\n",
      "     46        \u001b[36m0.3952\u001b[0m  0.0670\n",
      "     47        \u001b[36m0.3835\u001b[0m  0.0670\n",
      "     48        \u001b[36m0.3615\u001b[0m  0.0650\n",
      "     49        0.3625  0.0630\n",
      "     50        0.3656  0.0710\n",
      "     51        \u001b[36m0.3416\u001b[0m  0.0630\n",
      "     52        0.3566  0.0630\n",
      "     53        0.3419  0.0650\n",
      "     54        \u001b[36m0.3352\u001b[0m  0.0660\n",
      "     55        \u001b[36m0.3232\u001b[0m  0.0620\n",
      "     56        0.3278  0.0630\n",
      "     57        \u001b[36m0.3132\u001b[0m  0.0640\n",
      "     58        \u001b[36m0.2999\u001b[0m  0.0630\n",
      "     59        \u001b[36m0.2856\u001b[0m  0.0630\n",
      "     60        0.2887  0.0650\n",
      "     61        0.2907  0.0650\n",
      "     62        0.3220  0.0640\n",
      "     63        0.3217  0.0650\n",
      "     64        \u001b[36m0.2845\u001b[0m  0.0640\n",
      "     65        0.3068  0.0640\n",
      "     66        0.2886  0.0630\n",
      "     67        \u001b[36m0.2831\u001b[0m  0.0650\n",
      "     68        0.2843  0.0650\n",
      "     69        \u001b[36m0.2724\u001b[0m  0.0650\n",
      "     70        0.3108  0.0670\n",
      "     71        0.2878  0.0640\n",
      "     72        0.3024  0.0660\n",
      "     73        \u001b[36m0.2477\u001b[0m  0.0670\n",
      "     74        0.2958  0.0640\n",
      "     75        0.2715  0.0650\n",
      "     76        0.2911  0.0660\n",
      "     77        0.2857  0.0630\n",
      "     78        0.3031  0.0660\n",
      "     79        \u001b[36m0.2466\u001b[0m  0.0660\n",
      "     80        0.2606  0.0630\n",
      "     81        \u001b[36m0.2447\u001b[0m  0.0670\n",
      "     82        0.2710  0.0690\n",
      "     83        0.2581  0.0670\n",
      "     84        0.2490  0.0620\n",
      "     85        0.2916  0.0660\n",
      "     86        \u001b[36m0.2441\u001b[0m  0.0770\n",
      "     87        0.2471  0.0670\n",
      "     88        0.2585  0.0670\n",
      "     89        0.2518  0.0680\n",
      "     90        \u001b[36m0.2356\u001b[0m  0.0690\n",
      "     91        0.2568  0.0710\n",
      "     92        \u001b[36m0.2308\u001b[0m  0.0680\n",
      "     93        \u001b[36m0.2255\u001b[0m  0.0690\n",
      "     94        0.2399  0.0700\n",
      "     95        0.2379  0.0720\n",
      "     96        0.2509  0.0650\n",
      "     97        0.2528  0.0670\n",
      "     98        \u001b[36m0.2160\u001b[0m  0.0680\n",
      "     99        0.2527  0.0750\n",
      "    100        0.2325  0.0670\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0690\n",
      "      2       37.3047  0.0700\n",
      "      3       37.3047  0.0700\n",
      "      4       37.3047  0.0620\n",
      "      5       37.3047  0.0660\n",
      "      6       37.3047  0.0650\n",
      "      7       37.3047  0.0660\n",
      "      8       37.3047  0.0670\n",
      "      9       37.3047  0.0630\n",
      "     10       37.3047  0.0640\n",
      "     11       37.3047  0.0620\n",
      "     12       37.3047  0.0670\n",
      "     13       37.3047  0.0620\n",
      "     14       37.3047  0.0620\n",
      "     15       37.3047  0.0630\n",
      "     16       37.3047  0.0640\n",
      "     17       37.3047  0.0650\n",
      "     18       37.3047  0.0660\n",
      "     19       37.3047  0.0620\n",
      "     20       37.3047  0.0650\n",
      "     21       37.3047  0.0620\n",
      "     22       \u001b[36m19.6669\u001b[0m  0.0650\n",
      "     23        \u001b[36m0.5980\u001b[0m  0.0650\n",
      "     24        \u001b[36m0.5310\u001b[0m  0.0650\n",
      "     25        \u001b[36m0.5194\u001b[0m  0.0660\n",
      "     26        \u001b[36m0.4897\u001b[0m  0.0610\n",
      "     27        \u001b[36m0.4790\u001b[0m  0.0640\n",
      "     28        \u001b[36m0.4618\u001b[0m  0.0630\n",
      "     29        0.4645  0.0620\n",
      "     30        \u001b[36m0.4424\u001b[0m  0.0660\n",
      "     31        0.4501  0.0650\n",
      "     32        \u001b[36m0.4199\u001b[0m  0.0640\n",
      "     33        \u001b[36m0.3915\u001b[0m  0.0610\n",
      "     34        0.3991  0.0640\n",
      "     35        \u001b[36m0.3737\u001b[0m  0.0620\n",
      "     36        \u001b[36m0.3725\u001b[0m  0.0650\n",
      "     37        0.3906  0.0620\n",
      "     38        \u001b[36m0.3526\u001b[0m  0.0600\n",
      "     39        0.3625  0.0640\n",
      "     40        \u001b[36m0.3439\u001b[0m  0.0620\n",
      "     41        0.3529  0.0650\n",
      "     42        \u001b[36m0.3019\u001b[0m  0.0760\n",
      "     43        0.3120  0.0690\n",
      "     44        \u001b[36m0.2914\u001b[0m  0.0650\n",
      "     45        0.3275  0.0630\n",
      "     46        0.2980  0.0690\n",
      "     47        0.3232  0.0660\n",
      "     48        \u001b[36m0.2890\u001b[0m  0.0620\n",
      "     49        0.3245  0.0660\n",
      "     50        \u001b[36m0.2570\u001b[0m  0.0620\n",
      "     51        \u001b[36m0.2484\u001b[0m  0.0630\n",
      "     52        0.2973  0.0700\n",
      "     53        0.2731  0.0700\n",
      "     54        0.2842  0.0690\n",
      "     55        \u001b[36m0.2448\u001b[0m  0.0690\n",
      "     56        0.2976  0.0670\n",
      "     57        0.2764  0.0660\n",
      "     58        0.2839  0.0670\n",
      "     59        0.2583  0.0640\n",
      "     60        0.2654  0.0650\n",
      "     61        0.2669  0.0650\n",
      "     62        0.2520  0.0650\n",
      "     63        0.2485  0.0670\n",
      "     64        0.2649  0.0630\n",
      "     65        0.2542  0.0670\n",
      "     66        \u001b[36m0.2384\u001b[0m  0.0620\n",
      "     67        0.2718  0.0670\n",
      "     68        0.2452  0.0690\n",
      "     69        0.2404  0.0720\n",
      "     70        0.2407  0.0640\n",
      "     71        \u001b[36m0.2099\u001b[0m  0.0650\n",
      "     72        0.2742  0.0630\n",
      "     73        0.2359  0.0640\n",
      "     74        0.2231  0.0650\n",
      "     75        0.2314  0.0670\n",
      "     76        0.2503  0.0660\n",
      "     77        0.2379  0.0620\n",
      "     78        0.2468  0.0640\n",
      "     79        0.2372  0.0620\n",
      "     80        0.2358  0.0620\n",
      "     81        0.2221  0.0680\n",
      "     82        0.2375  0.0620\n",
      "     83        0.2163  0.0630\n",
      "     84        0.2219  0.0660\n",
      "     85        0.2417  0.0620\n",
      "     86        0.2493  0.0680\n",
      "     87        0.2562  0.0660\n",
      "     88        0.2230  0.0630\n",
      "     89        0.2244  0.0660\n",
      "     90        0.2256  0.0630\n",
      "     91        0.2120  0.0660\n",
      "     92        0.2159  0.0650\n",
      "     93        0.2206  0.0660\n",
      "     94        0.2197  0.0630\n",
      "     95        0.2371  0.0680\n",
      "     96        \u001b[36m0.1943\u001b[0m  0.0610\n",
      "     97        0.2320  0.0620\n",
      "     98        0.2382  0.0650\n",
      "     99        0.2266  0.0660\n",
      "    100        0.2081  0.0630\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0620\n",
      "      2       37.3047  0.0630\n",
      "      3       37.3047  0.0640\n",
      "      4       37.3047  0.0610\n",
      "      5       37.3047  0.0610\n",
      "      6       37.3047  0.0650\n",
      "      7       37.3047  0.0610\n",
      "      8       37.3047  0.0650\n",
      "      9       37.3047  0.0650\n",
      "     10       37.3047  0.0640\n",
      "     11       37.3047  0.0630\n",
      "     12       37.3047  0.0610\n",
      "     13       37.3047  0.0640\n",
      "     14       37.3047  0.0640\n",
      "     15       37.3047  0.0620\n",
      "     16       37.3047  0.0630\n",
      "     17       37.3047  0.0640\n",
      "     18       37.3047  0.0620\n",
      "     19       37.3047  0.0600\n",
      "     20       37.3047  0.0610\n",
      "     21       37.3047  0.0580\n",
      "     22       37.3047  0.0650\n",
      "     23       37.3047  0.0620\n",
      "     24       37.3047  0.0600\n",
      "     25       37.3047  0.0620\n",
      "     26       37.3047  0.0650\n",
      "     27       37.3047  0.0670\n",
      "     28       \u001b[36m20.6344\u001b[0m  0.0610\n",
      "     29        \u001b[36m0.6211\u001b[0m  0.0630\n",
      "     30        \u001b[36m0.5878\u001b[0m  0.0640\n",
      "     31        \u001b[36m0.5583\u001b[0m  0.0680\n",
      "     32        \u001b[36m0.5471\u001b[0m  0.0600\n",
      "     33        \u001b[36m0.5327\u001b[0m  0.0610\n",
      "     34        \u001b[36m0.5034\u001b[0m  0.0660\n",
      "     35        \u001b[36m0.4918\u001b[0m  0.0650\n",
      "     36        \u001b[36m0.4712\u001b[0m  0.0620\n",
      "     37        \u001b[36m0.4675\u001b[0m  0.0650\n",
      "     38        \u001b[36m0.4400\u001b[0m  0.0630\n",
      "     39        \u001b[36m0.4185\u001b[0m  0.0600\n",
      "     40        \u001b[36m0.4180\u001b[0m  0.0640\n",
      "     41        \u001b[36m0.4015\u001b[0m  0.0630\n",
      "     42        \u001b[36m0.3964\u001b[0m  0.0630\n",
      "     43        \u001b[36m0.3943\u001b[0m  0.0600\n",
      "     44        \u001b[36m0.3831\u001b[0m  0.0610\n",
      "     45        \u001b[36m0.3766\u001b[0m  0.0640\n",
      "     46        0.4007  0.0630\n",
      "     47        \u001b[36m0.3586\u001b[0m  0.0600\n",
      "     48        0.3635  0.0620\n",
      "     49        0.3689  0.0630\n",
      "     50        \u001b[36m0.3352\u001b[0m  0.0640\n",
      "     51        0.3443  0.0620\n",
      "     52        \u001b[36m0.3251\u001b[0m  0.0640\n",
      "     53        0.3412  0.0660\n",
      "     54        \u001b[36m0.3093\u001b[0m  0.0620\n",
      "     55        0.3240  0.0600\n",
      "     56        \u001b[36m0.2847\u001b[0m  0.0620\n",
      "     57        0.3205  0.0630\n",
      "     58        \u001b[36m0.2810\u001b[0m  0.0610\n",
      "     59        0.2890  0.0600\n",
      "     60        0.2854  0.0590\n",
      "     61        0.2917  0.0590\n",
      "     62        \u001b[36m0.2792\u001b[0m  0.0620\n",
      "     63        0.2808  0.0640\n",
      "     64        0.3335  0.0650\n",
      "     65        \u001b[36m0.2607\u001b[0m  0.0640\n",
      "     66        0.2847  0.0650\n",
      "     67        0.2744  0.0610\n",
      "     68        0.3039  0.0670\n",
      "     69        0.2854  0.0680\n",
      "     70        0.2756  0.0660\n",
      "     71        \u001b[36m0.2573\u001b[0m  0.0630\n",
      "     72        0.3058  0.0670\n",
      "     73        0.2862  0.0630\n",
      "     74        0.2777  0.0680\n",
      "     75        0.2806  0.0670\n",
      "     76        0.2759  0.0670\n",
      "     77        0.2731  0.0660\n",
      "     78        \u001b[36m0.2558\u001b[0m  0.0660\n",
      "     79        0.2618  0.0630\n",
      "     80        0.2735  0.0650\n",
      "     81        \u001b[36m0.2507\u001b[0m  0.0660\n",
      "     82        0.2542  0.0640\n",
      "     83        0.2837  0.0670\n",
      "     84        0.2818  0.0640\n",
      "     85        \u001b[36m0.2408\u001b[0m  0.0660\n",
      "     86        0.2557  0.0650\n",
      "     87        0.2643  0.0660\n",
      "     88        \u001b[36m0.2165\u001b[0m  0.0660\n",
      "     89        0.2289  0.0610\n",
      "     90        0.2707  0.0640\n",
      "     91        0.2409  0.0620\n",
      "     92        0.2236  0.0620\n",
      "     93        0.2621  0.0650\n",
      "     94        0.2427  0.0640\n",
      "     95        0.2407  0.0650\n",
      "     96        0.2424  0.0640\n",
      "     97        0.2339  0.0680\n",
      "     98        0.2401  0.0650\n",
      "     99        0.2375  0.0660\n",
      "    100        0.2707  0.0640\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0600\n",
      "      2       37.3047  0.0640\n",
      "      3       37.3047  0.0630\n",
      "      4       37.3047  0.0620\n",
      "      5       37.3047  0.0650\n",
      "      6       37.3047  0.0600\n",
      "      7       37.3047  0.0630\n",
      "      8       37.3047  0.0620\n",
      "      9       37.3047  0.1020\n",
      "     10       37.3047  0.0670\n",
      "     11       37.3047  0.0630\n",
      "     12       37.3047  0.0610\n",
      "     13       37.3047  0.0630\n",
      "     14       37.3047  0.0620\n",
      "     15       37.3047  0.0620\n",
      "     16       37.3047  0.0620\n",
      "     17       37.3047  0.0630\n",
      "     18       37.3047  0.0640\n",
      "     19       37.3047  0.0650\n",
      "     20       37.3047  0.0610\n",
      "     21       37.3047  0.0580\n",
      "     22       \u001b[36m37.1405\u001b[0m  0.0640\n",
      "     23       \u001b[36m30.2732\u001b[0m  0.0640\n",
      "     24       \u001b[36m13.8609\u001b[0m  0.0600\n",
      "     25        \u001b[36m0.5848\u001b[0m  0.0640\n",
      "     26        \u001b[36m0.5513\u001b[0m  0.0660\n",
      "     27        0.5661  0.0650\n",
      "     28        0.5558  0.0630\n",
      "     29        \u001b[36m0.5320\u001b[0m  0.0630\n",
      "     30        \u001b[36m0.5258\u001b[0m  0.0620\n",
      "     31        \u001b[36m0.5177\u001b[0m  0.0590\n",
      "     32        \u001b[36m0.5106\u001b[0m  0.0620\n",
      "     33        0.5160  0.0610\n",
      "     34        \u001b[36m0.5019\u001b[0m  0.0590\n",
      "     35        \u001b[36m0.5019\u001b[0m  0.0640\n",
      "     36        \u001b[36m0.4986\u001b[0m  0.0660\n",
      "     37        \u001b[36m0.4892\u001b[0m  0.0620\n",
      "     38        \u001b[36m0.4758\u001b[0m  0.0620\n",
      "     39        \u001b[36m0.4658\u001b[0m  0.0640\n",
      "     40        \u001b[36m0.4537\u001b[0m  0.0620\n",
      "     41        \u001b[36m0.4507\u001b[0m  0.0620\n",
      "     42        \u001b[36m0.4388\u001b[0m  0.0670\n",
      "     43        \u001b[36m0.4202\u001b[0m  0.0640\n",
      "     44        \u001b[36m0.4171\u001b[0m  0.0610\n",
      "     45        \u001b[36m0.4123\u001b[0m  0.0620\n",
      "     46        0.4156  0.0630\n",
      "     47        0.4145  0.0640\n",
      "     48        0.4177  0.0590\n",
      "     49        \u001b[36m0.3962\u001b[0m  0.0610\n",
      "     50        0.4157  0.0620\n",
      "     51        \u001b[36m0.3814\u001b[0m  0.0630\n",
      "     52        0.3902  0.0620\n",
      "     53        0.3872  0.0660\n",
      "     54        \u001b[36m0.3494\u001b[0m  0.0630\n",
      "     55        0.3771  0.0630\n",
      "     56        0.3627  0.0650\n",
      "     57        \u001b[36m0.3414\u001b[0m  0.0620\n",
      "     58        0.3541  0.0630\n",
      "     59        0.3522  0.0640\n",
      "     60        \u001b[36m0.3317\u001b[0m  0.0640\n",
      "     61        0.3334  0.0610\n",
      "     62        0.3641  0.0630\n",
      "     63        0.3455  0.0630\n",
      "     64        0.3318  0.0620\n",
      "     65        0.3581  0.0620\n",
      "     66        0.3594  0.0610\n",
      "     67        0.3493  0.0620\n",
      "     68        0.3488  0.0580\n",
      "     69        0.3510  0.0600\n",
      "     70        \u001b[36m0.3121\u001b[0m  0.0610\n",
      "     71        0.3252  0.0610\n",
      "     72        0.3834  0.0610\n",
      "     73        0.3266  0.0650\n",
      "     74        0.3570  0.0640\n",
      "     75        0.3288  0.0650\n",
      "     76        0.3527  0.0620\n",
      "     77        \u001b[36m0.3031\u001b[0m  0.0650\n",
      "     78        0.3562  0.0620\n",
      "     79        0.3243  0.0650\n",
      "     80        0.3314  0.0580\n",
      "     81        0.3313  0.0590\n",
      "     82        0.3686  0.0640\n",
      "     83        0.3195  0.0620\n",
      "     84        0.3243  0.0720\n",
      "     85        0.3433  0.0630\n",
      "     86        \u001b[36m0.2904\u001b[0m  0.0660\n",
      "     87        0.3438  0.0670\n",
      "     88        0.2990  0.0610\n",
      "     89        0.2948  0.0640\n",
      "     90        0.3161  0.0660\n",
      "     91        0.2987  0.0620\n",
      "     92        0.2932  0.0630\n",
      "     93        0.3245  0.0640\n",
      "     94        0.2915  0.0610\n",
      "     95        0.3000  0.0630\n",
      "     96        \u001b[36m0.2871\u001b[0m  0.0680\n",
      "     97        0.3249  0.0630\n",
      "     98        0.3330  0.0660\n",
      "     99        0.2929  0.0630\n",
      "    100        0.3576  0.0670\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0680\n",
      "      2       37.3047  0.0650\n",
      "      3       37.3047  0.0590\n",
      "      4       37.3047  0.0640\n",
      "      5       37.3047  0.0640\n",
      "      6       37.3047  0.0660\n",
      "      7       37.3047  0.0650\n",
      "      8       37.3047  0.0650\n",
      "      9       37.3047  0.0600\n",
      "     10       37.3047  0.0690\n",
      "     11       37.3047  0.0740\n",
      "     12       37.3047  0.0700\n",
      "     13       37.3047  0.0690\n",
      "     14       37.3047  0.0720\n",
      "     15       37.3047  0.0680\n",
      "     16       37.3047  0.0670\n",
      "     17       37.3047  0.0620\n",
      "     18       37.3047  0.0610\n",
      "     19       37.3047  0.0650\n",
      "     20       37.3047  0.0650\n",
      "     21       37.3047  0.0590\n",
      "     22       37.3047  0.0680\n",
      "     23       \u001b[36m23.2969\u001b[0m  0.0650\n",
      "     24        \u001b[36m0.6336\u001b[0m  0.0700\n",
      "     25        \u001b[36m0.5827\u001b[0m  0.0750\n",
      "     26        0.5844  0.1080\n",
      "     27        0.5850  0.0750\n",
      "     28        \u001b[36m0.5785\u001b[0m  0.0710\n",
      "     29        \u001b[36m0.5682\u001b[0m  0.0690\n",
      "     30        0.5721  0.0720\n",
      "     31        \u001b[36m0.5607\u001b[0m  0.0720\n",
      "     32        0.5668  0.0700\n",
      "     33        \u001b[36m0.5580\u001b[0m  0.0730\n",
      "     34        0.5601  0.0710\n",
      "     35        0.5617  0.0710\n",
      "     36        \u001b[36m0.5516\u001b[0m  0.0670\n",
      "     37        \u001b[36m0.5421\u001b[0m  0.0690\n",
      "     38        0.5523  0.0680\n",
      "     39        0.5507  0.0680\n",
      "     40        0.5438  0.0650\n",
      "     41        0.5466  0.0590\n",
      "     42        \u001b[36m0.5381\u001b[0m  0.0660\n",
      "     43        \u001b[36m0.5381\u001b[0m  0.0630\n",
      "     44        \u001b[36m0.5368\u001b[0m  0.0680\n",
      "     45        \u001b[36m0.5238\u001b[0m  0.0660\n",
      "     46        0.5292  0.0660\n",
      "     47        0.5296  0.0650\n",
      "     48        0.5472  0.0730\n",
      "     49        0.5272  0.0670\n",
      "     50        0.5296  0.0710\n",
      "     51        0.5447  0.0660\n",
      "     52        0.5400  0.0640\n",
      "     53        0.5279  0.0680\n",
      "     54        \u001b[36m0.5199\u001b[0m  0.0680\n",
      "     55        0.5387  0.0640\n",
      "     56        0.5310  0.0670\n",
      "     57        0.5296  0.0660\n",
      "     58        \u001b[36m0.5085\u001b[0m  0.0690\n",
      "     59        0.5166  0.0680\n",
      "     60        0.5207  0.0660\n",
      "     61        0.5358  0.0680\n",
      "     62        0.5101  0.0640\n",
      "     63        0.5091  0.0680\n",
      "     64        0.5255  0.0680\n",
      "     65        0.5382  0.0670\n",
      "     66        \u001b[36m0.4918\u001b[0m  0.0840\n",
      "     67        0.5033  0.0640\n",
      "     68        0.5391  0.0660\n",
      "     69        0.5216  0.0650\n",
      "     70        0.5199  0.0660\n",
      "     71        0.4989  0.0630\n",
      "     72        0.5005  0.0650\n",
      "     73        \u001b[36m0.4870\u001b[0m  0.0730\n",
      "     74        0.5437  0.0680\n",
      "     75        0.5228  0.0660\n",
      "     76        0.5040  0.0670\n",
      "     77        0.5065  0.0690\n",
      "     78        0.4975  0.0700\n",
      "     79        0.5018  0.0690\n",
      "     80        0.5070  0.0720\n",
      "     81        0.5002  0.0670\n",
      "     82        0.4981  0.0660\n",
      "     83        0.5076  0.0660\n",
      "     84        \u001b[36m0.4771\u001b[0m  0.0630\n",
      "     85        0.5189  0.0660\n",
      "     86        \u001b[36m0.4740\u001b[0m  0.0640\n",
      "     87        0.5069  0.0640\n",
      "     88        0.4913  0.0640\n",
      "     89        \u001b[36m0.4661\u001b[0m  0.0670\n",
      "     90        0.4999  0.0630\n",
      "     91        0.5054  0.0610\n",
      "     92        \u001b[36m0.4602\u001b[0m  0.0630\n",
      "     93        0.4717  0.0650\n",
      "     94        0.4714  0.0640\n",
      "     95        0.4908  0.0630\n",
      "     96        0.4716  0.0650\n",
      "     97        \u001b[36m0.4558\u001b[0m  0.0650\n",
      "     98        0.4743  0.0630\n",
      "     99        0.4663  0.0630\n",
      "    100        \u001b[36m0.4344\u001b[0m  0.0630\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2320\u001b[0m  0.0640\n",
      "      2       37.2320  0.0660\n",
      "      3       37.2320  0.0620\n",
      "      4       37.2320  0.0620\n",
      "      5       37.2320  0.0660\n",
      "      6       37.2320  0.0630\n",
      "      7       37.2320  0.0630\n",
      "      8       37.2320  0.0640\n",
      "      9       37.2320  0.0690\n",
      "     10       37.2320  0.0650\n",
      "     11       37.2320  0.0690\n",
      "     12       37.2320  0.0650\n",
      "     13       37.2320  0.0630\n",
      "     14       37.2320  0.0650\n",
      "     15       37.2320  0.0680\n",
      "     16       37.2320  0.0630\n",
      "     17       37.2320  0.0630\n",
      "     18       37.2320  0.0670\n",
      "     19       37.2320  0.0650\n",
      "     20       37.2320  0.0670\n",
      "     21       37.2320  0.0750\n",
      "     22       37.2320  0.0740\n",
      "     23       37.2320  0.0740\n",
      "     24       \u001b[36m37.0681\u001b[0m  0.0660\n",
      "     25       \u001b[36m20.0934\u001b[0m  0.0660\n",
      "     26        \u001b[36m0.5893\u001b[0m  0.0640\n",
      "     27        0.5948  0.0650\n",
      "     28        \u001b[36m0.5596\u001b[0m  0.0650\n",
      "     29        \u001b[36m0.5214\u001b[0m  0.0670\n",
      "     30        0.5233  0.0610\n",
      "     31        \u001b[36m0.5104\u001b[0m  0.0590\n",
      "     32        \u001b[36m0.4918\u001b[0m  0.0650\n",
      "     33        0.5076  0.0670\n",
      "     34        \u001b[36m0.4763\u001b[0m  0.0650\n",
      "     35        \u001b[36m0.4506\u001b[0m  0.0650\n",
      "     36        0.4586  0.0650\n",
      "     37        0.4521  0.0630\n",
      "     38        \u001b[36m0.4152\u001b[0m  0.0680\n",
      "     39        0.4207  0.0660\n",
      "     40        0.4200  0.0660\n",
      "     41        \u001b[36m0.3860\u001b[0m  0.0750\n",
      "     42        0.4093  0.0720\n",
      "     43        0.4037  0.0700\n",
      "     44        \u001b[36m0.3704\u001b[0m  0.0730\n",
      "     45        0.3812  0.0710\n",
      "     46        0.3760  0.0710\n",
      "     47        \u001b[36m0.3617\u001b[0m  0.0740\n",
      "     48        \u001b[36m0.3459\u001b[0m  0.0690\n",
      "     49        0.3733  0.0700\n",
      "     50        0.3998  0.1010\n",
      "     51        \u001b[36m0.3381\u001b[0m  0.1120\n",
      "     52        0.3761  0.0940\n",
      "     53        0.3381  0.0650\n",
      "     54        0.3604  0.0700\n",
      "     55        0.3545  0.0660\n",
      "     56        0.3680  0.0640\n",
      "     57        0.3472  0.0680\n",
      "     58        0.3413  0.0660\n",
      "     59        0.3646  0.0700\n",
      "     60        \u001b[36m0.3125\u001b[0m  0.0710\n",
      "     61        0.3339  0.0710\n",
      "     62        0.3413  0.0690\n",
      "     63        0.3388  0.0680\n",
      "     64        0.3592  0.0730\n",
      "     65        0.3193  0.0680\n",
      "     66        0.3175  0.0790\n",
      "     67        0.3495  0.1550\n",
      "     68        0.3374  0.0800\n",
      "     69        0.3450  0.0880\n",
      "     70        0.3215  0.0990\n",
      "     71        0.3291  0.0865\n",
      "     72        0.3259  0.0830\n",
      "     73        0.3133  0.0750\n",
      "     74        0.3486  0.0780\n",
      "     75        0.3178  0.0750\n",
      "     76        0.3324  0.0770\n",
      "     77        0.3336  0.0770\n",
      "     78        0.3167  0.0746\n",
      "     79        0.3274  0.0740\n",
      "     80        0.3442  0.0730\n",
      "     81        0.3318  0.0710\n",
      "     82        0.3380  0.0710\n",
      "     83        0.3446  0.0720\n",
      "     84        0.3397  0.0710\n",
      "     85        0.3231  0.0760\n",
      "     86        0.3360  0.0740\n",
      "     87        \u001b[36m0.3111\u001b[0m  0.0750\n",
      "     88        0.3334  0.0725\n",
      "     89        0.3175  0.0740\n",
      "     90        \u001b[36m0.3056\u001b[0m  0.0720\n",
      "     91        0.3518  0.0770\n",
      "     92        0.3299  0.0750\n",
      "     93        0.3271  0.0680\n",
      "     94        0.3330  0.0720\n",
      "     95        0.3122  0.0710\n",
      "     96        0.3139  0.0730\n",
      "     97        0.3324  0.0740\n",
      "     98        0.3746  0.0670\n",
      "     99        0.3059  0.0690\n",
      "    100        0.3185  0.0660\n"
     ]
    }
   ],
   "source": [
    "resultados = cross_val_score(classificador_sklearn, previsores, classes, cv = 10, scoring = 'accuracy') # Cross validation = cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8804824561403507, 0.044266207799744314)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media = resultados.mean()\n",
    "desvio = resultados.std()\n",
    "media, desvio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85964912, 0.8245614 , 0.84210526, 0.94736842, 0.89473684,\n",
       "       0.92982456, 0.84210526, 0.94736842, 0.84210526, 0.875     ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "748b4bbb40e1dc38591514fffe1d01ec0f0710858425568fad37763828692ea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
